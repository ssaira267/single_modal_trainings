{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6e4f2e",
   "metadata": {},
   "source": [
    "# Rock Class Prediction using XGBoost\n",
    "## Image Model: Feature Extraction from FMS Image (VGG19) and Final Rock Class Prediction using XGBoost\n",
    "## Tabular Model: Rock Classification using Wireline Log Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, image_folder, image_id_column, class_column, target_classes):\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    images = []\n",
    "    for image_id in data[image_id_column]:\n",
    "        img_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "        img = load_img(img_path, target_size=image_size)\n",
    "        img_array = img_to_array(img).flatten()\n",
    "        images.append(img_array)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[class_column] = label_encoder.fit_transform(data[class_column])\n",
    "\n",
    "    labels = data[class_column]\n",
    "\n",
    "    return data, images, labels\n",
    "\n",
    "def extract_image_features(data, image_folder, image_id_column, cnn_model):\n",
    "    extracted_features = []\n",
    "    image_ids = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        image_id = row[image_id_column]\n",
    "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image_array = img_to_array(image)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = image_array / 255.0\n",
    "\n",
    "        features = cnn_model.predict(image_array)\n",
    "\n",
    "        extracted_features.append(features.flatten())\n",
    "        image_ids.append(image_id)\n",
    "\n",
    "    extracted_features = np.array(extracted_features)\n",
    "    image_ids = np.array(image_ids)\n",
    "\n",
    "    return extracted_features, image_ids\n",
    "\n",
    "def train_xgboost_model(X_train, y_train, X_val, y_val):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(X_train_resampled, y_train_resampled, \n",
    "              eval_set=[(X_train_resampled, y_train_resampled), (X_val, y_val)], \n",
    "              eval_metric=[\"merror\", \"mlogloss\"], \n",
    "              early_stopping_rounds=10, \n",
    "              verbose=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    train_logloss = history['validation_0']['mlogloss']\n",
    "    val_logloss = history['validation_1']['mlogloss']\n",
    "    train_error = history['validation_0']['merror']\n",
    "    val_error = history['validation_1']['merror']\n",
    "    epochs = range(len(train_logloss))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_logloss, label='Train')\n",
    "    plt.plot(epochs, val_logloss, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.legend()\n",
    "    plt.title('XGBoost Log Loss')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_error, label='Train')\n",
    "    plt.plot(epochs, val_error, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.legend()\n",
    "    plt.title('XGBoost Classification Error')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the CSV file\n",
    "def preprocess_data(csv_path, image_folder, image_id_column, class_column, target_classes):\n",
    "    data, images, labels = load_data(csv_path, image_folder, image_id_column, class_column, target_classes)\n",
    "\n",
    "    well_log_data = data.drop(columns=['IMAGE_ID', 'DEPTH_WMSF (m)'])\n",
    "\n",
    "    return data, images, labels, well_log_data\n",
    "\n",
    "# Load the pre-trained VGG19 model\n",
    "def load_cnn_model(input_shape=(224, 224, 3)):\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    cnn_model = Model(inputs=base_model.input, outputs=x)\n",
    "    return cnn_model\n",
    "\n",
    "# Train and evaluate image model\n",
    "def train_and_evaluate_image_model(data, images, labels, cnn_model):\n",
    "    X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    X_train_img, X_val_img, y_train_img, y_val_img = train_test_split(X_train_img, y_train_img, test_size=0.25, random_state=42)\n",
    "\n",
    "    extracted_features, _ = extract_image_features(data, image_folder, image_id_column, cnn_model)\n",
    "\n",
    "    image_model = train_xgboost_model(X_train_img, y_train_img, X_val_img, y_val_img)\n",
    "    plot_training_curves(image_model.evals_result())\n",
    "    evaluate_model(image_model, X_test_img, y_test_img)\n",
    "\n",
    "# Train and evaluate tabular model\n",
    "def train_and_evaluate_tabular_model(well_log_data, labels):\n",
    "    X_train_tabular, X_test_tabular, y_train_tabular, y_test_tabular = train_test_split(well_log_data, labels, test_size=0.2, random_state=42)\n",
    "    X_train_tabular, X_val_tabular, y_train_tabular, y_val_tabular = train_test_split(X_train_tabular, y_train_tabular, test_size=0.25, random_state=42)\n",
    "\n",
    "    tabular_model = train_xgboost_model(X_train_tabular, y_train_tabular, X_val_tabular, y_val_tabular)\n",
    "    plot_training_curves(tabular_model.evals_result())\n",
    "    evaluate_model(tabular_model, X_test_tabular, y_test_tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58778c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and columns\n",
    "csv_path = \"/path/to/csv/file.csv\"\n",
    "image_folder = \"/path/to/image/folder\"\n",
    "image_id_column = \"IMAGE_ID\" \n",
    "class_column = \"ROCK_CLASS\"\n",
    "target_classes = ['Wackestone', 'Packstone', 'Grainstone', 'Floatstone', 'Rudstone']\n",
    "\n",
    "# Define image size\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Preprocess data\n",
    "data, images, labels, well_log_data = preprocess_data(csv_path, image_folder, image_id_column, class_column, target_classes)\n",
    "\n",
    "# Load CNN model\n",
    "cnn_model = load_cnn_model()\n",
    "\n",
    "# Train and evaluate image model\n",
    "train_and_evaluate_image_model(data, images, labels, cnn_model)\n",
    "\n",
    "# Train and evaluate tabular model\n",
    "train_and_evaluate_tabular_model(well_log_data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
